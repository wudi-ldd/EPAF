{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "from segment_anything import sam_model_registry\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from lora import LoRA_sam  \n",
    "\n",
    "# Set random seed\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the image encoder of SAM\n",
    "### 1. Define the SAM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_type = 'vit_h'\n",
    "checkpoint = 'weights/sam_vit_h_4b8939.pth'\n",
    "device = 'cuda:0'\n",
    "\n",
    "# Load the SAM model and initialize LoRA\n",
    "sam_model = sam_model_registry[model_type](checkpoint=checkpoint)\n",
    "sam_model.to(device)\n",
    "\n",
    "# Initialize the LoRA_sam model\n",
    "r = 32  # Rank of LoRA\n",
    "lora_sam_model = LoRA_sam(sam_model, rank=r)\n",
    "lora_sam_model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read the training and validation set lists\n",
    "def read_split_files(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        file_names = f.read().strip().split('\\n')\n",
    "    return file_names\n",
    "\n",
    "# Dataset loading\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, sam_model, file_list, mask_size=(256, 256), device='cpu'):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.sam_model = sam_model\n",
    "        self.mask_size = mask_size\n",
    "        self.device = device\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.png') and f.replace('.png', '') in file_list]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Read image\n",
    "        image_file = self.image_files[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_file)\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (1024, 1024), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # Read mask\n",
    "        mask_file = image_file.replace('.png', '.png')\n",
    "        mask_path = os.path.join(self.mask_dir, mask_file)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
    "        mask = cv2.resize(mask, self.mask_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # Convert to torch tensor\n",
    "        input_image_torch = torch.as_tensor(image, dtype=torch.float32).to(self.device)\n",
    "        input_image_torch = input_image_torch.permute(2, 0, 1).contiguous()  # [C, H, W]\n",
    "\n",
    "        # Preprocessing step for SAM model\n",
    "        input_image = self.sam_model.preprocess(input_image_torch.to(self.device))\n",
    "\n",
    "        # Convert mask to torch tensor\n",
    "        mask = torch.as_tensor(mask, dtype=torch.long).to(self.device)  # Mask is single-channel\n",
    "\n",
    "        return input_image, mask\n",
    "\n",
    "# Create dataset instances for training and validation sets\n",
    "# Set paths\n",
    "image_dir = 'datasets/images'\n",
    "mask_dir = 'datasets/masks'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Read file name lists\n",
    "train_files = read_split_files('datasets/train.txt')\n",
    "val_files = read_split_files('datasets/val.txt')\n",
    "\n",
    "# Create dataset and data loader for training and validation sets\n",
    "train_dataset = SegmentationDataset(image_dir, mask_dir, sam_model, train_files, device=device)\n",
    "val_dataset = SegmentationDataset(image_dir, mask_dir, sam_model, val_files, device=device)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Test the data loader\n",
    "for i, (images, masks) in enumerate(train_loader):\n",
    "    print(f'Train Batch {i}:')\n",
    "    print(f'Images shape: {images.shape}')  # Should be [B, C, H, W]\n",
    "    print(f'Masks shape: {masks.shape}')    # Should be [B, H, W]\n",
    "    print(f'Mask unique values: {torch.unique(masks)}')  # Output unique values in the mask\n",
    "    break\n",
    "\n",
    "for i, (images, masks) in enumerate(val_loader):\n",
    "    print(f'Val Batch {i}:')\n",
    "    print(f'Images shape: {images.shape}')  # Should be [B, C, H, W]\n",
    "    print(f'Masks shape: {masks.shape}')    # Should be [B, H, W]\n",
    "    print(f'Mask unique values: {torch.unique(masks)}')  # Output unique values in the mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of Contrastive Center Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveCenterLoss(nn.Module):\n",
    "    \"\"\"Contrastive Center Loss.\n",
    "    \n",
    "    This loss combines the concepts of Center Loss and Contrastive Loss,\n",
    "    considering both intra-class compactness and inter-class separability.\n",
    "    \n",
    "    Parameters:\n",
    "        num_classes (int): Number of classes.\n",
    "        feat_dim (int): Dimension of features.\n",
    "        use_gpu (bool): Whether to use GPU.\n",
    "        lambda_c (float): Weight of the center loss part.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2, feat_dim=256, use_gpu=True, lambda_c=1.0):\n",
    "        super(ContrastiveCenterLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.feat_dim = feat_dim\n",
    "        self.use_gpu = use_gpu\n",
    "        self.lambda_c = lambda_c\n",
    "\n",
    "        # Initialize class centers\n",
    "        if self.use_gpu:\n",
    "            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).cuda())\n",
    "        else:\n",
    "            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim))\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        \"\"\"\n",
    "        Forward propagation function.\n",
    "        \n",
    "        Parameters:\n",
    "            x: Feature matrix, shape (batch_size, feat_dim).\n",
    "            labels: Ground truth labels, shape (batch_size).\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Compute distance between each feature and all class centers\n",
    "        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n",
    "                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n",
    "        distmat.addmm_(x, self.centers.t(), beta=1, alpha=-2)\n",
    "        \n",
    "        # Create a mask for the class labels\n",
    "        classes = torch.arange(self.num_classes).long()\n",
    "        if self.use_gpu: classes = classes.cuda()\n",
    "        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n",
    "        mask = labels.eq(classes.expand(batch_size, self.num_classes))\n",
    "\n",
    "        # Compute intra-class distances (distance to the correct class center)\n",
    "        intra_distances = distmat * mask.float()\n",
    "        intra_distances = intra_distances.sum() / batch_size\n",
    "\n",
    "        # Compute inter-class distances (distance to incorrect class centers)\n",
    "        inter_distances = distmat * (~mask).float()\n",
    "        inter_distances = inter_distances.sum() / (batch_size * (self.num_classes - 1))\n",
    "        \n",
    "        # Compute the contrastive center loss\n",
    "        loss = (self.lambda_c / 2.0) * intra_distances / (inter_distances + 1e-6) / 0.1\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to compute loss\n",
    "def compute_loss(class_logits, masks, upsampled_embedding, alpha, loss_fn, contrastive_center_loss, ce_weight=1.0, center_weight=1.0):\n",
    "    \"\"\"\n",
    "    Compute cross-entropy loss and contrastive center loss, and combine them with given weights.\n",
    "\n",
    "    Args:\n",
    "        class_logits (Tensor): Classification results (B, num_classes, 256, 256)\n",
    "        masks (Tensor): Masks (B, 256, 256)\n",
    "        upsampled_embedding (Tensor): Upsampled embeddings (B, 256, 256, 256)\n",
    "        alpha (float): Weight of contrastive center loss\n",
    "        loss_fn (nn.Module): Cross-entropy loss function\n",
    "        contrastive_center_loss (ContrastiveCenterLoss): Instance of contrastive center loss function\n",
    "        ce_weight (float): Weight of cross-entropy loss\n",
    "        center_weight (float): Weight of contrastive center loss\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Total loss\n",
    "        Tensor: Cross-entropy loss value\n",
    "        Tensor: Contrastive center loss value\n",
    "    \"\"\"\n",
    "    # Compute cross-entropy loss\n",
    "    loss_ce = loss_fn(class_logits, masks.long())\n",
    "    \n",
    "    # Compute contrastive center loss\n",
    "    loss_cent = contrastive_center_loss(upsampled_embedding.view(-1, 256), masks.view(-1)) * alpha\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = ce_weight * loss_ce + center_weight * loss_cent\n",
    "    \n",
    "    return total_loss, loss_ce.item(), loss_cent.item()\n",
    "\n",
    "# Configure logger\n",
    "logging.basicConfig(filename='logs/best_model_ce_cocenter_lora.log', level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define cross-entropy loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define a custom model with only one convolutional layer\n",
    "class FeatureMapper(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(FeatureMapper, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        return x\n",
    "    \n",
    "# Instantiate contrastive center loss and the custom model\n",
    "contrastive_center_loss = ContrastiveCenterLoss(num_classes=2, feat_dim=256, use_gpu=torch.cuda.is_available())\n",
    "model = FeatureMapper(in_channels=256, out_channels=2)\n",
    "model.to(device)\n",
    "\n",
    "# Freeze all SAM model parameters, unfreeze only LoRA layers and custom convolution layer\n",
    "for param in lora_sam_model.sam.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for layer in lora_sam_model.A_weights + lora_sam_model.B_weights:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(filter(lambda p: p.requires_grad, lora_sam_model.parameters())) + list(model.parameters()),\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "# Define a separate optimizer for contrastive center loss\n",
    "optimizer_centloss = torch.optim.Adam(contrastive_center_loss.parameters(), lr=0.5)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = 0\n",
    "checkpoint_path = 'logs/best_model_ce_lora.pth'  # Replace with actual path\n",
    "alpha = 0.5 # Weight for center loss\n",
    "\n",
    "# Define Warmup + custom cosine annealing learning rate scheduler\n",
    "warmup_epochs = 10  # Number of epochs for warmup\n",
    "min_lr_factor = 0.01  # Minimum learning rate is 1% of the maximum\n",
    "\n",
    "def lr_lambda(epoch):\n",
    "    if epoch < warmup_epochs:\n",
    "        return float(epoch / warmup_epochs)\n",
    "    else:\n",
    "        cosine_decay = 0.5 * (1 + torch.cos(torch.tensor(epoch - warmup_epochs) * torch.pi / (num_epochs - warmup_epochs)))\n",
    "        return float(min_lr_factor + (1 - min_lr_factor) * cosine_decay)\n",
    "    \n",
    "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "scheduler_centloss = lr_scheduler.LambdaLR(optimizer_centloss, lr_lambda=lr_lambda)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    try:\n",
    "        # Set model to training mode\n",
    "        lora_sam_model.train()\n",
    "        model.train()\n",
    "\n",
    "        total_loss = 0  # Accumulate total loss for each batch\n",
    "        total_loss_ce = 0  # Accumulate cross-entropy loss for each batch\n",
    "        total_loss_cent = 0  # Accumulate contrastive center loss for each batch\n",
    "        num_batches = 0\n",
    "\n",
    "        # Training phase\n",
    "        for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Train]\"):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            # Forward pass: Get image embeddings\n",
    "            image_embedding = lora_sam_model.sam.image_encoder(images)  # B, 256, 64, 64\n",
    "\n",
    "            # Upsample to (B, 256, 256, 256)\n",
    "            upsampled_embedding = F.interpolate(image_embedding, size=(256, 256), mode='bilinear', align_corners=False)\n",
    "\n",
    "            # Process embeddings using the custom model\n",
    "            class_logits = model(upsampled_embedding)  # B, num_classes, 256, 256\n",
    "\n",
    "            # Compute total loss, including cross-entropy loss and contrastive center loss\n",
    "            loss, loss_ce, loss_cent = compute_loss(\n",
    "                class_logits, masks, upsampled_embedding, alpha, loss_fn, contrastive_center_loss\n",
    "            )\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            optimizer.zero_grad()\n",
    "            optimizer_centloss.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # To eliminate alpha's influence on center point updates, multiply by (1./alpha)\n",
    "            for param in contrastive_center_loss.parameters():\n",
    "                if param.grad is not None:\n",
    "                    param.grad.data *= (1. / alpha)\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer_centloss.step()\n",
    "\n",
    "            # Accumulate losses\n",
    "            total_loss += loss.item()\n",
    "            total_loss_ce += loss_ce\n",
    "            total_loss_cent += loss_cent\n",
    "            num_batches += 1\n",
    "\n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step()\n",
    "        scheduler_centloss.step()\n",
    "\n",
    "        # Calculate average loss\n",
    "        avg_train_loss = total_loss / num_batches\n",
    "        avg_train_loss_ce = total_loss_ce / num_batches\n",
    "        avg_train_loss_cent = total_loss_cent / num_batches\n",
    "\n",
    "        # Set model to evaluation mode\n",
    "        lora_sam_model.eval()\n",
    "        model.eval()\n",
    "\n",
    "        val_loss = 0\n",
    "        val_loss_ce = 0\n",
    "        val_loss_cent = 0\n",
    "        num_val_batches = 0\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient calculation\n",
    "            for images, masks in tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Validation]\"):\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "                # Forward pass: Get image embeddings\n",
    "                image_embedding = lora_sam_model.sam.image_encoder(images)  # B, 256, 64, 64\n",
    "\n",
    "                # Upsample to (B, 256, 256, 256)\n",
    "                upsampled_embedding = F.interpolate(image_embedding, size=(256, 256), mode='bilinear', align_corners=False)\n",
    "\n",
    "                # Process embeddings using the custom model\n",
    "                class_logits = model(upsampled_embedding)  # B, num_classes, 256, 256\n",
    "\n",
    "                # Compute total loss, including cross-entropy loss and contrastive center loss\n",
    "                loss, loss_ce, loss_cent = compute_loss(\n",
    "                    class_logits, masks, upsampled_embedding, alpha, loss_fn, contrastive_center_loss\n",
    "                )\n",
    "\n",
    "                # Accumulate losses\n",
    "                val_loss += loss.item()\n",
    "                val_loss_ce += loss_ce\n",
    "                val_loss_cent += loss_cent\n",
    "                num_val_batches += 1\n",
    "\n",
    "        avg_val_loss = val_loss / num_val_batches\n",
    "        avg_val_loss_ce = val_loss_ce / num_val_batches\n",
    "        avg_val_loss_cent = val_loss_cent / num_val_batches\n",
    "\n",
    "        # Get current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        current_lr_centloss = optimizer_centloss.param_groups[0]['lr']\n",
    "\n",
    "        # Output and log training/validation losses and current learning rates\n",
    "        logging.info(f\"Epoch [{epoch + 1}/{num_epochs}], Learning Rate: {current_lr:.6f}, Center Loss Learning Rate: {current_lr_centloss:.6f}, \"\n",
    "                     f\"Average Train Loss: {avg_train_loss:.4f}, Average Val Loss: {avg_val_loss:.4f}, \"\n",
    "                     f\"Train CE Loss: {avg_train_loss_ce:.4f}, Train Center Loss: {avg_train_loss_cent:.4f}, \"\n",
    "                     f\"Val CE Loss: {avg_val_loss_ce:.4f}, Val Center Loss: {avg_val_loss_cent:.4f}\")\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Learning Rate: {current_lr:.6f}, Center Loss Learning Rate: {current_lr_centloss:.6f}, \"\n",
    "              f\"Average Train Loss: {avg_train_loss:.4f}, Average Val Loss: {avg_val_loss:.4f}, \"\n",
    "              f\"Train CE Loss: {avg_train_loss_ce:.4f}, Train Center Loss: {avg_train_loss_cent:.4f}, \"\n",
    "              f\"Val CE Loss: {avg_val_loss_ce:.4f}, Val Center Loss: {avg_val_loss_cent:.4f}\")\n",
    "\n",
    "\n",
    "        # Save the model with the best validation performance\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_epoch = epoch + 1\n",
    "            # Save LoRA and classifier weights\n",
    "            lora_sam_model.save_lora_parameters(f'logs/best_lora_cocenter_rank{r}.safetensors')\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "            }, checkpoint_path)\n",
    "            logging.info(f\"Best model saved at epoch {best_epoch} with val loss {best_val_loss:.4f}\")\n",
    "            print(f\"Best model saved at epoch {best_epoch} with val loss {best_val_loss:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Exception occurred during epoch {epoch + 1}: {str(e)}\")\n",
    "        print(f\"Exception occurred during epoch {epoch + 1}: {str(e)}\")\n",
    "        lora_sam_model.save_lora_parameters(f'logs/error_lora_epoch_{epoch + 1}.safetensors')\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "        }, f'logs/error_model_epoch_{epoch + 1}.pth')\n",
    "        break\n",
    "\n",
    "logging.info(\"Training completed\")\n",
    "print(\"Training completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
